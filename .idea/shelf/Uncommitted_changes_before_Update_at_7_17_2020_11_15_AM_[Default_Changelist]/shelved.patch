Index: evaluation_dqn.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/usr/bin/env python\r\nfrom __future__ import print_function\r\n\r\n# import tensorflow as tf\r\nfrom statistics import mean\r\n\r\nimport tensorflow.compat.v1 as tf\r\ntf.disable_v2_behavior()\r\nimport cv2\r\nimport sys\r\nimport atexit\r\n\r\nfrom game import wrapped_flappy_bird\r\n\r\nsys.path.append(\"game/\")\r\nimport game.wrapped_flappy_bird as game\r\nimport random\r\nimport numpy as np\r\nfrom collections import deque\r\n\r\noutStatement = ''\r\n\r\nGAME = 'bird' # the name of the game being played for log files\r\nACTIONS = 2 # number of valid actions\r\nGAMMA = 0.99 # decay rate of past observations\r\nDIFFICULTY = 'general'\r\nAGENT = 'DQN'\r\nNUMEBEROFGAMES = 2\r\n\r\n############################### UNCOMMENT THIS SECTION FOR TESTING\r\n# OBSERVE = 1000000. # timesteps to observe before training\r\n# EXPLORE = 2000000. # frames over which to anneal epsilon\r\n# FINAL_EPSILON = 0.0001 # final value of epsilon\r\n# INITIAL_EPSILON = 0.0001 # starting value of epsilon\r\noutputFile = \"/output_test.txt\"\r\na_file = open(\"logs_\" + GAME + \"/\" + AGENT + \"/\" + DIFFICULTY + \"/readout_test.txt\", 'a+')\r\nh_file = open(\"logs_\" + GAME + \"/\" + AGENT + \"/\" + DIFFICULTY + \"/hidden_test.txt\", 'a+')\r\nout_file = open(\"logs_\" + GAME + \"/\" + AGENT + \"/\" + DIFFICULTY + \"/output_test.txt\", 'a+')\r\n###############################\r\n\r\n# printing\r\n# outputFile = \"/output.txt\"\r\n# a_file = open(\"logs_\" + GAME + \"/\" + AGENT + \"/\" + DIFFICULTY + \"/readout.txt\", 'a+')\r\n# h_file = open(\"logs_\" + GAME + \"/\" + AGENT + \"/\" + DIFFICULTY + \"/hidden.txt\", 'a+')\r\n# out_file = open(\"logs_\" + GAME + \"/\" + AGENT + \"/\" + DIFFICULTY + \"/output.txt\", 'a+')\r\n# OBSERVE = 10000\r\n# # OBSERVE = easy\r\n# EXPLORE = 3000000\r\n# FINAL_EPSILON = 0.0001\r\n# INITIAL_EPSILON = 0.1\r\n# # INITIAL_EPSILON = 0.00009999999983673486\r\n# REPLAY_MEMORY = 50000 # number of previous transitions to remember\r\n# BATCH = 32 # size of minibatch\r\nFRAME_PER_ACTION = 1\r\n\r\ndef weight_variable(shape):\r\n    initial = tf.truncated_normal(shape, stddev = 0.01)\r\n    return tf.Variable(initial)\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.01, shape = shape)\r\n    return tf.Variable(initial)\r\n\r\ndef conv2d(x, W, stride):\r\n    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\r\n\r\ndef max_pool_2x2(x):\r\n    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\r\n\r\ndef createNetwork():\r\n    # network weights\r\n    W_conv1 = weight_variable([8, 8, 4, 32])\r\n    b_conv1 = bias_variable([32])\r\n\r\n    W_conv2 = weight_variable([4, 4, 32, 64])\r\n    b_conv2 = bias_variable([64])\r\n\r\n    W_conv3 = weight_variable([3, 3, 64, 64])\r\n    b_conv3 = bias_variable([64])\r\n\r\n    W_fc1 = weight_variable([1600, 512])\r\n    b_fc1 = bias_variable([512])\r\n\r\n    W_fc2 = weight_variable([512, ACTIONS])\r\n    b_fc2 = bias_variable([ACTIONS])\r\n\r\n    # input layer\r\n    s = tf.placeholder(\"float\", [None, 80, 80, 4])\r\n\r\n    # hidden layers\r\n    h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)\r\n    h_pool1 = max_pool_2x2(h_conv1)\r\n\r\n    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)\r\n    #h_pool2 = max_pool_2x2(h_conv2)\r\n\r\n    h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)\r\n    #h_pool3 = max_pool_2x2(h_conv3)\r\n\r\n    #h_pool3_flat = tf.reshape(h_pool3, [-1, 256])\r\n    h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])\r\n\r\n    h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)\r\n\r\n    # readout layer\r\n    readout = tf.matmul(h_fc1, W_fc2) + b_fc2\r\n\r\n    return s, readout, h_fc1\r\n\r\ndef trainNetwork(s, readout, h_fc1, sess):\r\n    # define the cost function\r\n\r\n    with open(\"logs_\" + GAME + \"/\" + AGENT + \"/\" + DIFFICULTY + outputFile, 'a') as fout:\r\n        fout.writelines(\"================================= NEW EVALUATION =================================\")\r\n        fout.writelines(\"\\n\")\r\n\r\n    a = tf.placeholder(\"float\", [None, ACTIONS])\r\n    y = tf.placeholder(\"float\", [None])\r\n    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\r\n    cost = tf.reduce_mean(tf.square(y - readout_action))\r\n    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\r\n\r\n    # open up a game state to communicate with emulator\r\n    game_state = game.GameState(difficulty=DIFFICULTY)\r\n\r\n    # store the previous observations in replay memory\r\n    # D = deque()\r\n    # get the first state by doing nothing and preprocess the image to 80x80x4\r\n    do_nothing = np.zeros(ACTIONS)\r\n    do_nothing[0] = 1\r\n    x_t, r_0, terminal, s1, s2 = game_state.frame_step(do_nothing)\r\n    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\r\n    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\r\n    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\r\n\r\n    # saving and loading networks\r\n    saver = tf.train.Saver()\r\n    sess.run(tf.initialize_all_variables())\r\n    checkpoint = tf.train.get_checkpoint_state(\"saved_networks/\" + AGENT + \"/\" + DIFFICULTY + \"/\")\r\n\r\n\r\n    ############################### UNCOMMENT THIS SECTION FOR TESTING\r\n    if checkpoint and checkpoint.model_checkpoint_path:\r\n        saver.restore(sess, checkpoint.model_checkpoint_path)\r\n        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\r\n    else:\r\n        print(\"Could not find old network weights\")\r\n\r\n    # start training\r\n    # epsilon = INITIAL_EPSILON\r\n    # random_action = 0\r\n    t = 0\r\n    scores = []\r\n    currentGame = 1\r\n    # while \"flappy bird\" != \"angry bird\":\r\n    while currentGame <= NUMEBEROFGAMES:\r\n        # choose an action epsilon greedily\r\n        readout_t = readout.eval(feed_dict={s : [s_t]})[0]\r\n        a_t = np.zeros([ACTIONS])\r\n        action_index = 0\r\n        if t % FRAME_PER_ACTION == 0:\r\n            action_index = np.argmax(readout_t)\r\n            a_t[action_index] = 1\r\n            # if random.random() <= epsilon:\r\n            #     print(\"----------Random Action----------\")\r\n            #     random_action = random_action + 1\r\n            #     action_index = random.randrange(ACTIONS)\r\n            #     a_t[random.randrange(ACTIONS)] = 1\r\n            # else:\r\n            #     action_index = np.argmax(readout_t)\r\n            #     a_t[action_index] = 1\r\n        else:\r\n            a_t[0] = 1 # do nothing\r\n\r\n        # scale down epsilon\r\n        # if epsilon > FINAL_EPSILON and t > OBSERVE:\r\n        #     epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\r\n\r\n        # run the selected action and observe next state and reward\r\n        # x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\r\n        x_t1_colored, r_t, terminal, score, final_score = game_state.frame_step(a_t)\r\n        # if(score > 0):\r\n        #     print (score)\r\n        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\r\n        ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\r\n        x_t1 = np.reshape(x_t1, (80, 80, 1))\r\n        #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)\r\n        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\r\n\r\n        # store the transition in D\r\n        # D.append((s_t, a_t, r_t, s_t1, terminal))\r\n        # if len(D) > REPLAY_MEMORY:\r\n        #     D.popleft()\r\n\r\n        # only train if done observing\r\n        # if t > OBSERVE:\r\n        #     # sample a minibatch to train on\r\n        #     minibatch = random.sample(D, BATCH)\r\n        #\r\n        #     # get the batch variables\r\n        #     s_j_batch = [d[0] for d in minibatch]\r\n        #     a_batch = [d[1] for d in minibatch]\r\n        #     r_batch = [d[2] for d in minibatch]\r\n        #     s_j1_batch = [d[3] for d in minibatch]\r\n        #\r\n        #     y_batch = []\r\n        #     readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\r\n        #     for i in range(0, len(minibatch)):\r\n        #         terminal = minibatch[i][4]\r\n        #         # if terminal, only equals reward\r\n        #         if terminal:\r\n        #             y_batch.append(r_batch[i])\r\n        #         else:\r\n        #             y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\r\n        #\r\n        #     # perform gradient step\r\n        #     train_step.run(feed_dict = {\r\n        #         y : y_batch,\r\n        #         a : a_batch,\r\n        #         s : s_j_batch}\r\n        #     )\r\n\r\n        # update the old values\r\n        s_t = s_t1\r\n        t += 1\r\n\r\n        # save progress every 10000 iterations\r\n        # if t % 10000 == 0:\r\n        #     saver.save(sess, 'saved_networks/' + AGENT + \"/\" + DIFFICULTY + \"/\" + GAME + '-dqn', global_step = t)\r\n\r\n        # print info\r\n        state = \"test\"\r\n        # if t <= OBSERVE:\r\n        #     state = \"observe\"\r\n        # elif t > OBSERVE and t <= OBSERVE + EXPLORE:\r\n        #     state = \"explore\"\r\n        # else:\r\n        #     state = \"train\"\r\n\r\n        print(\"TIMESTEP: \", t, \"/ GAME: \", currentGame, \"/ STATE: \", state, \"/ ACTION: \", action_index, \"/ SCORE: \", score)\r\n        # write info to files\r\n\r\n        # outStatement = \"TIMESTEP: \", str(t), \"/ STATE: \", state, \"/ EPSILON: \", str(epsilon), \"/ ACTION: \", str(action_index), \\\r\n        #        \"/ REWARD: \", str(r_t), \"/ SCORE: \", str(score), \\\r\n        #        \"/ Q_MAX: %e\" % np.max(readout_t), \"\\n\"\r\n        # # out_file.writelines(line)\r\n        # if(final_score > 0):\r\n        #     # print(\"score: \", score)\r\n        #     outStatement = \"TIMESTEP: \", str(t), \"/ STATE: \", state, \"/ EPSILON: \", str(epsilon), \"/ ACTION: \", \\\r\n        #                    str(action_index), \"/ REWARD: \", str(r_t), \"/ FINAL SCORE: \", str(final_score), \\\r\n        #                    \"/RANDOM ACTIONS: \", str(random_action),\"/ Q_MAX: %e\" % np.max(readout_t), \"\\n\"\r\n        #     with open(\"logs_\" + GAME + \"/\" + AGENT + \"/\" + DIFFICULTY + outputFile, 'a') as fout:\r\n        #         fout.writelines(outStatement)\r\n        if terminal:\r\n            outStatement = \"TIMESTEP: \", str(t), \"/ GAME: \", str(currentGame), \"/ ACTION: \", \\\r\n                           str(action_index), \"/ FINAL SCORE: \", str(final_score), \"\\n\"\r\n            with open(\"logs_\" + GAME + \"/\" + AGENT + \"/\" + DIFFICULTY + outputFile, 'a') as fout:\r\n                fout.writelines(outStatement)\r\n            scores.append(final_score)\r\n            currentGame = currentGame + 1\r\n\r\n        # if(score != 0 and score % 25 == 0):\r\n        #     with open(\"logs_\" + GAME + \"/\" + AGENT + \"/\" + DIFFICULTY + outputFile, 'a') as fout:\r\n        #         fout.writelines(outStatement)\r\n\r\n        # if t % 10000 <= easy:\r\n        #     a_file.write(\",\".join([str(x) for x in readout_t]) + '\\n')\r\n        #     h_file.write(\",\".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\\n')\r\n        #\r\n        #     cv2.imwrite(\"logs_tetris/frame\" + str(t) + \".png\", x_t1)\r\n\r\n    maxScore = max(scores)\r\n    minScore = min(scores)\r\n    meanScore = mean(scores)\r\n    lastOutStatement = \"Total TIMESTEP: \", str(t), \"/ total Games: \", str(NUMEBEROFGAMES), \"/ Maximum Score: \", \\\r\n                       str(maxScore), \"/ Minimum Score: \", str(minScore), \"/ Average Score: \", str(meanScore), \"\\n\"\r\n    with open(\"logs_\" + GAME + \"/\" + AGENT + \"/\" + DIFFICULTY + outputFile, 'a') as fout:\r\n        fout.writelines(lastOutStatement)\r\n\r\n\r\n\r\ndef playGame():\r\n    sess = tf.InteractiveSession()\r\n    # sess = tfc.InteractiveSession()\r\n    s, readout, h_fc1 = createNetwork()\r\n    trainNetwork(s, readout, h_fc1, sess)\r\n\r\ndef printLine():\r\n    print (\"outStatement: \", outStatement)\r\n\r\ndef main():\r\n    # try:\r\n    playGame()\r\n    atexit.register(printLine)\r\n    # finally:\r\n    #     with open(\"logs_\" + GAME + \"/output.txt\", 'a') as fout:\r\n    #         fout.writelines(outStatement)\r\n    #         print (\"outStatement: \", outStatement)\r\n    # except:\r\n    #     with open(\"logs_\" + GAME + \"/output.txt\", 'a') as fout:\r\n    #         fout.writelines(outStatement)\r\n    #         print (\"outStatement: \", outStatement)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- evaluation_dqn.py	(revision 5cb6e4555c639fd88bb40d57e95ba6d63abf924c)
+++ evaluation_dqn.py	(date 1594927452783)
@@ -25,32 +25,15 @@
 GAMMA = 0.99 # decay rate of past observations
 DIFFICULTY = 'general'
 AGENT = 'DQN'
-NUMEBEROFGAMES = 2
+NUMEBEROFGAMES = 100
 
 ############################### UNCOMMENT THIS SECTION FOR TESTING
-# OBSERVE = 1000000. # timesteps to observe before training
-# EXPLORE = 2000000. # frames over which to anneal epsilon
-# FINAL_EPSILON = 0.0001 # final value of epsilon
-# INITIAL_EPSILON = 0.0001 # starting value of epsilon
 outputFile = "/output_test.txt"
-a_file = open("logs_" + GAME + "/" + AGENT + "/" + DIFFICULTY + "/readout_test.txt", 'a+')
-h_file = open("logs_" + GAME + "/" + AGENT + "/" + DIFFICULTY + "/hidden_test.txt", 'a+')
+# a_file = open("logs_" + GAME + "/" + AGENT + "/" + DIFFICULTY + "/readout_test.txt", 'a+')
+# h_file = open("logs_" + GAME + "/" + AGENT + "/" + DIFFICULTY + "/hidden_test.txt", 'a+')
 out_file = open("logs_" + GAME + "/" + AGENT + "/" + DIFFICULTY + "/output_test.txt", 'a+')
 ###############################
 
-# printing
-# outputFile = "/output.txt"
-# a_file = open("logs_" + GAME + "/" + AGENT + "/" + DIFFICULTY + "/readout.txt", 'a+')
-# h_file = open("logs_" + GAME + "/" + AGENT + "/" + DIFFICULTY + "/hidden.txt", 'a+')
-# out_file = open("logs_" + GAME + "/" + AGENT + "/" + DIFFICULTY + "/output.txt", 'a+')
-# OBSERVE = 10000
-# # OBSERVE = easy
-# EXPLORE = 3000000
-# FINAL_EPSILON = 0.0001
-# INITIAL_EPSILON = 0.1
-# # INITIAL_EPSILON = 0.00009999999983673486
-# REPLAY_MEMORY = 50000 # number of previous transitions to remember
-# BATCH = 32 # size of minibatch
 FRAME_PER_ACTION = 1
 
 def weight_variable(shape):
@@ -107,7 +90,7 @@
 
     return s, readout, h_fc1
 
-def trainNetwork(s, readout, h_fc1, sess):
+def testNetwork(s, readout, h_fc1, sess):
     # define the cost function
 
     with open("logs_" + GAME + "/" + AGENT + "/" + DIFFICULTY + outputFile, 'a') as fout:
@@ -123,9 +106,6 @@
     # open up a game state to communicate with emulator
     game_state = game.GameState(difficulty=DIFFICULTY)
 
-    # store the previous observations in replay memory
-    # D = deque()
-    # get the first state by doing nothing and preprocess the image to 80x80x4
     do_nothing = np.zeros(ACTIONS)
     do_nothing[0] = 1
     x_t, r_0, terminal, s1, s2 = game_state.frame_step(do_nothing)
@@ -146,9 +126,6 @@
     else:
         print("Could not find old network weights")
 
-    # start training
-    # epsilon = INITIAL_EPSILON
-    # random_action = 0
     t = 0
     scores = []
     currentGame = 1
@@ -161,21 +138,8 @@
         if t % FRAME_PER_ACTION == 0:
             action_index = np.argmax(readout_t)
             a_t[action_index] = 1
-            # if random.random() <= epsilon:
-            #     print("----------Random Action----------")
-            #     random_action = random_action + 1
-            #     action_index = random.randrange(ACTIONS)
-            #     a_t[random.randrange(ACTIONS)] = 1
-            # else:
-            #     action_index = np.argmax(readout_t)
-            #     a_t[action_index] = 1
         else:
             a_t[0] = 1 # do nothing
-
-        # scale down epsilon
-        # if epsilon > FINAL_EPSILON and t > OBSERVE:
-        #     epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE
-
         # run the selected action and observe next state and reward
         # x_t1_colored, r_t, terminal = game_state.frame_step(a_t)
         x_t1_colored, r_t, terminal, score, final_score = game_state.frame_step(a_t)
@@ -187,70 +151,15 @@
         #s_t1 = np.append(x_t1, s_t[:,:,1:], axis = 2)
         s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)
 
-        # store the transition in D
-        # D.append((s_t, a_t, r_t, s_t1, terminal))
-        # if len(D) > REPLAY_MEMORY:
-        #     D.popleft()
-
-        # only train if done observing
-        # if t > OBSERVE:
-        #     # sample a minibatch to train on
-        #     minibatch = random.sample(D, BATCH)
-        #
-        #     # get the batch variables
-        #     s_j_batch = [d[0] for d in minibatch]
-        #     a_batch = [d[1] for d in minibatch]
-        #     r_batch = [d[2] for d in minibatch]
-        #     s_j1_batch = [d[3] for d in minibatch]
-        #
-        #     y_batch = []
-        #     readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})
-        #     for i in range(0, len(minibatch)):
-        #         terminal = minibatch[i][4]
-        #         # if terminal, only equals reward
-        #         if terminal:
-        #             y_batch.append(r_batch[i])
-        #         else:
-        #             y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))
-        #
-        #     # perform gradient step
-        #     train_step.run(feed_dict = {
-        #         y : y_batch,
-        #         a : a_batch,
-        #         s : s_j_batch}
-        #     )
-
         # update the old values
         s_t = s_t1
         t += 1
 
-        # save progress every 10000 iterations
-        # if t % 10000 == 0:
-        #     saver.save(sess, 'saved_networks/' + AGENT + "/" + DIFFICULTY + "/" + GAME + '-dqn', global_step = t)
-
         # print info
         state = "test"
-        # if t <= OBSERVE:
-        #     state = "observe"
-        # elif t > OBSERVE and t <= OBSERVE + EXPLORE:
-        #     state = "explore"
-        # else:
-        #     state = "train"
-
         print("TIMESTEP: ", t, "/ GAME: ", currentGame, "/ STATE: ", state, "/ ACTION: ", action_index, "/ SCORE: ", score)
         # write info to files
 
-        # outStatement = "TIMESTEP: ", str(t), "/ STATE: ", state, "/ EPSILON: ", str(epsilon), "/ ACTION: ", str(action_index), \
-        #        "/ REWARD: ", str(r_t), "/ SCORE: ", str(score), \
-        #        "/ Q_MAX: %e" % np.max(readout_t), "\n"
-        # # out_file.writelines(line)
-        # if(final_score > 0):
-        #     # print("score: ", score)
-        #     outStatement = "TIMESTEP: ", str(t), "/ STATE: ", state, "/ EPSILON: ", str(epsilon), "/ ACTION: ", \
-        #                    str(action_index), "/ REWARD: ", str(r_t), "/ FINAL SCORE: ", str(final_score), \
-        #                    "/RANDOM ACTIONS: ", str(random_action),"/ Q_MAX: %e" % np.max(readout_t), "\n"
-        #     with open("logs_" + GAME + "/" + AGENT + "/" + DIFFICULTY + outputFile, 'a') as fout:
-        #         fout.writelines(outStatement)
         if terminal:
             outStatement = "TIMESTEP: ", str(t), "/ GAME: ", str(currentGame), "/ ACTION: ", \
                            str(action_index), "/ FINAL SCORE: ", str(final_score), "\n"
@@ -259,16 +168,6 @@
             scores.append(final_score)
             currentGame = currentGame + 1
 
-        # if(score != 0 and score % 25 == 0):
-        #     with open("logs_" + GAME + "/" + AGENT + "/" + DIFFICULTY + outputFile, 'a') as fout:
-        #         fout.writelines(outStatement)
-
-        # if t % 10000 <= easy:
-        #     a_file.write(",".join([str(x) for x in readout_t]) + '\n')
-        #     h_file.write(",".join([str(x) for x in h_fc1.eval(feed_dict={s:[s_t]})[0]]) + '\n')
-        #
-        #     cv2.imwrite("logs_tetris/frame" + str(t) + ".png", x_t1)
-
     maxScore = max(scores)
     minScore = min(scores)
     meanScore = mean(scores)
@@ -281,9 +180,8 @@
 
 def playGame():
     sess = tf.InteractiveSession()
-    # sess = tfc.InteractiveSession()
     s, readout, h_fc1 = createNetwork()
-    trainNetwork(s, readout, h_fc1, sess)
+    testNetwork(s, readout, h_fc1, sess)
 
 def printLine():
     print ("outStatement: ", outStatement)
@@ -292,14 +190,6 @@
     # try:
     playGame()
     atexit.register(printLine)
-    # finally:
-    #     with open("logs_" + GAME + "/output.txt", 'a') as fout:
-    #         fout.writelines(outStatement)
-    #         print ("outStatement: ", outStatement)
-    # except:
-    #     with open("logs_" + GAME + "/output.txt", 'a') as fout:
-    #         fout.writelines(outStatement)
-    #         print ("outStatement: ", outStatement)
 
 
 if __name__ == "__main__":
Index: logs_bird/DQN/general/output_test.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>================================= NEW EVALUATION =================================\r\nTIMESTEP: 18/ GAME: 1/ ACTION: 0/ FINAL SCORE: 0\r\nTIMESTEP: 37/ GAME: 2/ ACTION: 0/ FINAL SCORE: 0\r\nTotal TIMESTEP: 37/ total Games: 2/ Maximum Score: 0/ Minimum Score: 0/ Average Score: 0\r\n================================= NEW EVALUATION =================================\r\nTIMESTEP: 18/ GAME: 1/ ACTION: 0/ FINAL SCORE: 0\r\nTIMESTEP: 37/ GAME: 2/ ACTION: 0/ FINAL SCORE: 0\r\nTotal TIMESTEP: 37/ total Games: 2/ Maximum Score: 0/ Minimum Score: 0/ Average Score: 0\r\n================================= NEW EVALUATION =================================\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- logs_bird/DQN/general/output_test.txt	(revision 5cb6e4555c639fd88bb40d57e95ba6d63abf924c)
+++ logs_bird/DQN/general/output_test.txt	(date 1594970209511)
@@ -7,3 +7,105 @@
 TIMESTEP: 37/ GAME: 2/ ACTION: 0/ FINAL SCORE: 0
 Total TIMESTEP: 37/ total Games: 2/ Maximum Score: 0/ Minimum Score: 0/ Average Score: 0
 ================================= NEW EVALUATION =================================
+================================= NEW EVALUATION =================================
+TIMESTEP: 16372/ GAME: 1/ ACTION: 1/ FINAL SCORE: 441
+TIMESTEP: 21287/ GAME: 2/ ACTION: 0/ FINAL SCORE: 132
+TIMESTEP: 23687/ GAME: 3/ ACTION: 0/ FINAL SCORE: 64
+TIMESTEP: 24365/ GAME: 4/ ACTION: 0/ FINAL SCORE: 17
+TIMESTEP: 50888/ GAME: 5/ ACTION: 0/ FINAL SCORE: 716
+TIMESTEP: 56449/ GAME: 6/ ACTION: 0/ FINAL SCORE: 149
+TIMESTEP: 61788/ GAME: 7/ ACTION: 0/ FINAL SCORE: 143
+TIMESTEP: 66801/ GAME: 8/ ACTION: 0/ FINAL SCORE: 134
+TIMESTEP: 71088/ GAME: 9/ ACTION: 0/ FINAL SCORE: 115
+TIMESTEP: 75856/ GAME: 10/ ACTION: 0/ FINAL SCORE: 128
+TIMESTEP: 85842/ GAME: 11/ ACTION: 0/ FINAL SCORE: 269
+TIMESTEP: 88196/ GAME: 12/ ACTION: 0/ FINAL SCORE: 62
+TIMESTEP: 90115/ GAME: 13/ ACTION: 0/ FINAL SCORE: 51
+TIMESTEP: 101825/ GAME: 14/ ACTION: 1/ FINAL SCORE: 315
+TIMESTEP: 105657/ GAME: 15/ ACTION: 1/ FINAL SCORE: 102
+TIMESTEP: 112846/ GAME: 16/ ACTION: 0/ FINAL SCORE: 193
+TIMESTEP: 129952/ GAME: 17/ ACTION: 0/ FINAL SCORE: 461
+TIMESTEP: 159267/ GAME: 18/ ACTION: 0/ FINAL SCORE: 791
+TIMESTEP: 160113/ GAME: 19/ ACTION: 0/ FINAL SCORE: 22
+TIMESTEP: 165304/ GAME: 20/ ACTION: 0/ FINAL SCORE: 139
+TIMESTEP: 168069/ GAME: 21/ ACTION: 0/ FINAL SCORE: 74
+TIMESTEP: 168233/ GAME: 22/ ACTION: 0/ FINAL SCORE: 3
+TIMESTEP: 168695/ GAME: 23/ ACTION: 1/ FINAL SCORE: 11
+TIMESTEP: 169651/ GAME: 24/ ACTION: 0/ FINAL SCORE: 25
+TIMESTEP: 174663/ GAME: 25/ ACTION: 0/ FINAL SCORE: 134
+TIMESTEP: 180973/ GAME: 26/ ACTION: 0/ FINAL SCORE: 169
+TIMESTEP: 181225/ GAME: 27/ ACTION: 0/ FINAL SCORE: 6
+TIMESTEP: 181369/ GAME: 28/ ACTION: 0/ FINAL SCORE: 3
+TIMESTEP: 187507/ GAME: 29/ ACTION: 0/ FINAL SCORE: 165
+TIMESTEP: 188686/ GAME: 30/ ACTION: 0/ FINAL SCORE: 31
+TIMESTEP: 193108/ GAME: 31/ ACTION: 0/ FINAL SCORE: 118
+TIMESTEP: 194895/ GAME: 32/ ACTION: 0/ FINAL SCORE: 47
+TIMESTEP: 197137/ GAME: 33/ ACTION: 0/ FINAL SCORE: 59
+TIMESTEP: 204161/ GAME: 34/ ACTION: 0/ FINAL SCORE: 189
+TIMESTEP: 206081/ GAME: 35/ ACTION: 0/ FINAL SCORE: 51
+TIMESTEP: 210331/ GAME: 36/ ACTION: 0/ FINAL SCORE: 114
+TIMESTEP: 247953/ GAME: 37/ ACTION: 0/ FINAL SCORE: 1016
+TIMESTEP: 267360/ GAME: 38/ ACTION: 0/ FINAL SCORE: 523
+TIMESTEP: 269686/ GAME: 39/ ACTION: 0/ FINAL SCORE: 62
+TIMESTEP: 273619/ GAME: 40/ ACTION: 0/ FINAL SCORE: 105
+TIMESTEP: 279718/ GAME: 41/ ACTION: 0/ FINAL SCORE: 164
+TIMESTEP: 280107/ GAME: 42/ ACTION: 0/ FINAL SCORE: 9
+TIMESTEP: 290342/ GAME: 43/ ACTION: 0/ FINAL SCORE: 276
+TIMESTEP: 294719/ GAME: 44/ ACTION: 0/ FINAL SCORE: 117
+TIMESTEP: 305111/ GAME: 45/ ACTION: 0/ FINAL SCORE: 280
+TIMESTEP: 305531/ GAME: 46/ ACTION: 0/ FINAL SCORE: 10
+TIMESTEP: 305779/ GAME: 47/ ACTION: 0/ FINAL SCORE: 6
+TIMESTEP: 305986/ GAME: 48/ ACTION: 0/ FINAL SCORE: 4
+TIMESTEP: 309199/ GAME: 49/ ACTION: 0/ FINAL SCORE: 86
+TIMESTEP: 313734/ GAME: 50/ ACTION: 1/ FINAL SCORE: 121
+TIMESTEP: 318223/ GAME: 51/ ACTION: 0/ FINAL SCORE: 120
+TIMESTEP: 325989/ GAME: 52/ ACTION: 0/ FINAL SCORE: 209
+TIMESTEP: 329277/ GAME: 53/ ACTION: 0/ FINAL SCORE: 88
+TIMESTEP: 329899/ GAME: 54/ ACTION: 0/ FINAL SCORE: 16
+TIMESTEP: 336503/ GAME: 55/ ACTION: 0/ FINAL SCORE: 177
+TIMESTEP: 339178/ GAME: 56/ ACTION: 0/ FINAL SCORE: 71
+TIMESTEP: 343185/ GAME: 57/ ACTION: 0/ FINAL SCORE: 107
+TIMESTEP: 346251/ GAME: 58/ ACTION: 0/ FINAL SCORE: 82
+TIMESTEP: 363849/ GAME: 59/ ACTION: 0/ FINAL SCORE: 474
+TIMESTEP: 371724/ GAME: 60/ ACTION: 0/ FINAL SCORE: 212
+TIMESTEP: 374012/ GAME: 61/ ACTION: 0/ FINAL SCORE: 61
+TIMESTEP: 398536/ GAME: 62/ ACTION: 0/ FINAL SCORE: 662
+TIMESTEP: 399343/ GAME: 63/ ACTION: 0/ FINAL SCORE: 21
+TIMESTEP: 404213/ GAME: 64/ ACTION: 0/ FINAL SCORE: 130
+TIMESTEP: 409868/ GAME: 65/ ACTION: 0/ FINAL SCORE: 152
+TIMESTEP: 423442/ GAME: 66/ ACTION: 0/ FINAL SCORE: 366
+TIMESTEP: 425082/ GAME: 67/ ACTION: 0/ FINAL SCORE: 43
+TIMESTEP: 431032/ GAME: 68/ ACTION: 0/ FINAL SCORE: 160
+TIMESTEP: 433430/ GAME: 69/ ACTION: 0/ FINAL SCORE: 64
+TIMESTEP: 446916/ GAME: 70/ ACTION: 0/ FINAL SCORE: 363
+TIMESTEP: 447202/ GAME: 71/ ACTION: 0/ FINAL SCORE: 7
+TIMESTEP: 449063/ GAME: 72/ ACTION: 0/ FINAL SCORE: 49
+TIMESTEP: 449194/ GAME: 73/ ACTION: 0/ FINAL SCORE: 2
+TIMESTEP: 455794/ GAME: 74/ ACTION: 0/ FINAL SCORE: 177
+TIMESTEP: 457803/ GAME: 75/ ACTION: 0/ FINAL SCORE: 53
+TIMESTEP: 462126/ GAME: 76/ ACTION: 0/ FINAL SCORE: 116
+TIMESTEP: 467613/ GAME: 77/ ACTION: 0/ FINAL SCORE: 147
+TIMESTEP: 472159/ GAME: 78/ ACTION: 0/ FINAL SCORE: 122
+TIMESTEP: 497719/ GAME: 79/ ACTION: 0/ FINAL SCORE: 690
+TIMESTEP: 503296/ GAME: 80/ ACTION: 1/ FINAL SCORE: 150
+TIMESTEP: 511299/ GAME: 81/ ACTION: 0/ FINAL SCORE: 215
+TIMESTEP: 516510/ GAME: 82/ ACTION: 0/ FINAL SCORE: 140
+TIMESTEP: 521796/ GAME: 83/ ACTION: 0/ FINAL SCORE: 142
+TIMESTEP: 524546/ GAME: 84/ ACTION: 0/ FINAL SCORE: 73
+TIMESTEP: 525864/ GAME: 85/ ACTION: 0/ FINAL SCORE: 34
+TIMESTEP: 542657/ GAME: 86/ ACTION: 0/ FINAL SCORE: 453
+TIMESTEP: 543058/ GAME: 87/ ACTION: 0/ FINAL SCORE: 10
+TIMESTEP: 549137/ GAME: 88/ ACTION: 0/ FINAL SCORE: 163
+TIMESTEP: 553817/ GAME: 89/ ACTION: 0/ FINAL SCORE: 125
+TIMESTEP: 555589/ GAME: 90/ ACTION: 0/ FINAL SCORE: 47
+TIMESTEP: 570925/ GAME: 91/ ACTION: 0/ FINAL SCORE: 413
+TIMESTEP: 579668/ GAME: 92/ ACTION: 0/ FINAL SCORE: 235
+TIMESTEP: 582288/ GAME: 93/ ACTION: 1/ FINAL SCORE: 70
+TIMESTEP: 583466/ GAME: 94/ ACTION: 1/ FINAL SCORE: 31
+TIMESTEP: 584865/ GAME: 95/ ACTION: 0/ FINAL SCORE: 37
+TIMESTEP: 585117/ GAME: 96/ ACTION: 0/ FINAL SCORE: 6
+TIMESTEP: 586868/ GAME: 97/ ACTION: 0/ FINAL SCORE: 46
+TIMESTEP: 590490/ GAME: 98/ ACTION: 0/ FINAL SCORE: 97
+TIMESTEP: 591981/ GAME: 99/ ACTION: 0/ FINAL SCORE: 39
+TIMESTEP: 597321/ GAME: 100/ ACTION: 1/ FINAL SCORE: 143
+Total TIMESTEP: 597321/ total Games: 100/ Maximum Score: 1016/ Minimum Score: 2/ Average Score: 160.32
Index: .idea/Code by Hossein.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<module type=\"PYTHON_MODULE\" version=\"4\">\r\n  <component name=\"NewModuleRootManager\">\r\n    <content url=\"file://$MODULE_DIR$\" />\r\n    <orderEntry type=\"jdk\" jdkName=\"Python 3.7\" jdkType=\"Python SDK\" />\r\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\r\n  </component>\r\n  <component name=\"PyDocumentationSettings\">\r\n    <option name=\"format\" value=\"PLAIN\" />\r\n    <option name=\"myDocStringFormat\" value=\"Plain\" />\r\n    <option name=\"renderExternalDocumentation\" value=\"true\" />\r\n  </component>\r\n  <component name=\"TestRunnerService\">\r\n    <option name=\"PROJECT_TEST_RUNNER\" value=\"pytest\" />\r\n  </component>\r\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/Code by Hossein.iml	(revision 5cb6e4555c639fd88bb40d57e95ba6d63abf924c)
+++ .idea/Code by Hossein.iml	(date 1594927208222)
@@ -2,7 +2,7 @@
 <module type="PYTHON_MODULE" version="4">
   <component name="NewModuleRootManager">
     <content url="file://$MODULE_DIR$" />
-    <orderEntry type="jdk" jdkName="Python 3.7" jdkType="Python SDK" />
+    <orderEntry type="jdk" jdkName="Python 3.7 (Tensorflow)" jdkType="Python SDK" />
     <orderEntry type="sourceFolder" forTests="false" />
   </component>
   <component name="PyDocumentationSettings">
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"JavaScriptSettings\">\r\n    <option name=\"languageLevel\" value=\"ES6\" />\r\n  </component>\r\n  <component name=\"ProjectRootManager\" version=\"2\" project-jdk-name=\"Python 3.7\" project-jdk-type=\"Python SDK\" />\r\n  <component name=\"PythonCompatibilityInspectionAdvertiser\">\r\n    <option name=\"version\" value=\"3\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- .idea/misc.xml	(revision 5cb6e4555c639fd88bb40d57e95ba6d63abf924c)
+++ .idea/misc.xml	(date 1594927208437)
@@ -3,7 +3,7 @@
   <component name="JavaScriptSettings">
     <option name="languageLevel" value="ES6" />
   </component>
-  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.7" project-jdk-type="Python SDK" />
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.7 (Tensorflow)" project-jdk-type="Python SDK" />
   <component name="PythonCompatibilityInspectionAdvertiser">
     <option name="version" value="3" />
   </component>
diff --git .idea/shelf/Uncommitted_changes_before_Update_at_7_13_2020_11_56_AM__Default_Changelist_.xml .idea/shelf/Uncommitted_changes_before_Update_at_7_13_2020_11_56_AM__Default_Changelist_.xml
